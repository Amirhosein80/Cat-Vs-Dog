{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "def  ImportImages (train_path = \"\" ,test_path = \"\"):\n",
    "\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1.0/255 ,\n",
    "        rotation_range=45 ,\n",
    "        width_shift_range=0.2 ,\n",
    "        height_shift_range=0.2 ,\n",
    "        shear_range=0.2 ,\n",
    "        zoom_range=0.2 ,\n",
    "        horizontal_flip=True)\n",
    "    \n",
    "    test_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_path ,\n",
    "        target_size=(200 ,200),\n",
    "        batch_size=32,\n",
    "        class_mode='binary'\n",
    "    )\n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "        test_path ,\n",
    "        target_size=(200 ,200),\n",
    "        batch_size=32,\n",
    "        class_mode='binary'\n",
    "    )\n",
    "\n",
    "    return (train_generator ,test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense ,Dropout ,MaxPooling2D ,Conv2D ,Flatten ,BatchNormalization ,add\n",
    "from keras.layers.experimental.preprocessing import RandomFlip ,RandomRotation ,Rescaling\n",
    "from keras.models import Model ,Sequential\n",
    "from keras import Input\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import binary_crossentropy\n",
    "from tensorflow import expand_dims\n",
    "import kerastuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 images belonging to 2 classes.\n",
      "Found 5000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_data ,test_data = ImportImages(train_path='./train' ,test_path='./test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Xception_layer(hp):\n",
    "    inputs = Input(shape=(200 ,200 ,3))\n",
    "    num_block = hp.Int('num_block', min_value=2, max_value=5, step=1)\n",
    "    num_filters = hp.Int('num_filters', min_value=8, max_value=32, step=8)\n",
    "    layer = Conv2D(num_filters ,(num_block,num_block) ,activation='relu' ,padding='same')(inputs)\n",
    "    layer = BatchNormalization()(layer)\n",
    "    layer = MaxPooling2D((2,2))(layer)\n",
    "    previous_block_layer = layer\n",
    "    \n",
    "    for _ in range(4) :\n",
    "        num_filters_1 = hp.Int('num_filters_1', min_value=16, max_value=64, step=16)\n",
    "        layer = Conv2D(num_filters_1 ,(3,3) ,activation='relu' ,padding='same')(layer)\n",
    "        layer = BatchNormalization()(layer)\n",
    "        \n",
    "        num_filters_2 = hp.Int('num_filters_2', min_value=32, max_value=128, step=16)\n",
    "        layer = Conv2D(num_filters_2 ,(3,3) ,activation='relu' ,padding='same')(layer)\n",
    "        layer = BatchNormalization()(layer)\n",
    "        layer = MaxPooling2D((2,2))(layer)\n",
    "        \n",
    "        layer2 = Conv2D(num_filters_2 ,(3,3) ,activation='relu' ,padding='same')(previous_block_layer)\n",
    "        layer2 = BatchNormalization()(layer2)\n",
    "        layer2 = MaxPooling2D((2,2))(layer2)\n",
    "        \n",
    "        layer = add([layer ,layer2])\n",
    "        previous_block_layer = layer\n",
    "    \n",
    "    layer = Flatten()(layer)\n",
    "    layer = Dropout(hp.Float('dense_dropout', min_value=0., max_value=0.7))(layer)\n",
    "    layer = Dense(units=hp.Int('num_dense_units', min_value=32, max_value=256, step=32),activation=\"relu\")(layer)\n",
    "    outputs = Dense(1 ,activation=\"sigmoid\")(layer)\n",
    "    model = Model(inputs ,outputs)\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    lr = hp.Choice('learning_rate' ,[0.001 ,0.0001 ,0.00001])\n",
    "    model.compile(loss=binary_crossentropy,\n",
    "             optimizer=Adam(learning_rate=lr),\n",
    "             metrics=['acc'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 200, 200, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 200, 200, 8)  104         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 200, 200, 8)  32          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 100, 100, 8)  0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 100, 100, 16) 1168        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 100, 100, 16) 64          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 100, 100, 32) 4640        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 100, 100, 32) 2336        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 100, 100, 32) 128         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 100, 100, 32) 128         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 50, 50, 32)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 50, 50, 32)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 50, 50, 32)   0           max_pooling2d_1[0][0]            \n",
      "                                                                 max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 50, 50, 16)   4624        add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 50, 50, 16)   64          conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 50, 50, 32)   4640        batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 50, 50, 32)   9248        add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 50, 50, 32)   128         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 50, 50, 32)   128         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 25, 25, 32)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 25, 25, 32)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 25, 25, 32)   0           max_pooling2d_3[0][0]            \n",
      "                                                                 max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 25, 25, 16)   4624        add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 25, 25, 16)   64          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 25, 25, 32)   4640        batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 25, 25, 32)   9248        add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 25, 25, 32)   128         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 25, 25, 32)   128         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 12, 12, 32)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 12, 12, 32)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 12, 12, 32)   0           max_pooling2d_5[0][0]            \n",
      "                                                                 max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 12, 12, 16)   4624        add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 12, 12, 16)   64          conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 12, 12, 32)   4640        batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 12, 12, 32)   9248        add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 12, 12, 32)   128         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 12, 12, 32)   128         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 6, 6, 32)     0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)  (None, 6, 6, 32)     0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 6, 6, 32)     0           max_pooling2d_7[0][0]            \n",
      "                                                                 max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 1152)         0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 1152)         0           flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 32)           36896       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            33          dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 102,025\n",
      "Trainable params: 101,369\n",
      "Non-trainable params: 656\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "tuner = kt.BayesianOptimization( Xception_layer, objective = 'val_acc', max_trials=10 ,overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 10m 09s]\n",
      "val_acc: 0.784600019454956\n",
      "\n",
      "Best val_acc So Far: 0.8270000219345093\n",
      "Total elapsed time: 01h 45m 38s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner.search(train_data ,validation_data = test_data ,epochs=10 ,steps_per_epoch =200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<kerastuner.engine.hyperparameters.HyperParameters at 0x1c9a2034be0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "best_hps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in .\\untitled_project\n",
      "Showing 10 best trials\n",
      "Objective(name='val_acc', direction='max')\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_block: 2\n",
      "num_filters: 32\n",
      "num_filters_1: 16\n",
      "num_filters_2: 128\n",
      "dense_dropout: 0.0\n",
      "num_dense_units: 32\n",
      "learning_rate: 0.001\n",
      "Score: 0.8270000219345093\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_block: 2\n",
      "num_filters: 16\n",
      "num_filters_1: 32\n",
      "num_filters_2: 128\n",
      "dense_dropout: 0.6884907542252953\n",
      "num_dense_units: 96\n",
      "learning_rate: 0.001\n",
      "Score: 0.8001999855041504\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_block: 2\n",
      "num_filters: 32\n",
      "num_filters_1: 64\n",
      "num_filters_2: 128\n",
      "dense_dropout: 0.0\n",
      "num_dense_units: 32\n",
      "learning_rate: 0.001\n",
      "Score: 0.784600019454956\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_block: 2\n",
      "num_filters: 8\n",
      "num_filters_1: 64\n",
      "num_filters_2: 128\n",
      "dense_dropout: 0.7\n",
      "num_dense_units: 256\n",
      "learning_rate: 0.001\n",
      "Score: 0.7789999842643738\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_block: 2\n",
      "num_filters: 32\n",
      "num_filters_1: 32\n",
      "num_filters_2: 128\n",
      "dense_dropout: 0.5802761745839086\n",
      "num_dense_units: 256\n",
      "learning_rate: 0.001\n",
      "Score: 0.7784000039100647\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_block: 3\n",
      "num_filters: 24\n",
      "num_filters_1: 32\n",
      "num_filters_2: 64\n",
      "dense_dropout: 0.03503898719192001\n",
      "num_dense_units: 256\n",
      "learning_rate: 0.0001\n",
      "Score: 0.7748000025749207\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_block: 2\n",
      "num_filters: 16\n",
      "num_filters_1: 48\n",
      "num_filters_2: 96\n",
      "dense_dropout: 0.4670584678248596\n",
      "num_dense_units: 96\n",
      "learning_rate: 0.0001\n",
      "Score: 0.7694000005722046\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_block: 3\n",
      "num_filters: 16\n",
      "num_filters_1: 32\n",
      "num_filters_2: 96\n",
      "dense_dropout: 0.6362960715090913\n",
      "num_dense_units: 96\n",
      "learning_rate: 0.001\n",
      "Score: 0.7630000114440918\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_block: 2\n",
      "num_filters: 16\n",
      "num_filters_1: 32\n",
      "num_filters_2: 64\n",
      "dense_dropout: 0.15952182340100407\n",
      "num_dense_units: 192\n",
      "learning_rate: 0.0001\n",
      "Score: 0.7559999823570251\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_block: 5\n",
      "num_filters: 16\n",
      "num_filters_1: 48\n",
      "num_filters_2: 96\n",
      "dense_dropout: 0.49089470232532645\n",
      "num_dense_units: 64\n",
      "learning_rate: 1e-05\n",
      "Score: 0.5896000266075134\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 200, 200, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 200, 200, 32) 416         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 200, 200, 32) 128         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 100, 100, 32) 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 100, 100, 16) 4624        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 100, 100, 16) 64          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 100, 100, 128 18560       batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 100, 100, 128 36992       max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 100, 100, 128 512         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 100, 100, 128 512         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 50, 50, 128)  0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 50, 50, 128)  0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 50, 50, 128)  0           max_pooling2d_1[0][0]            \n",
      "                                                                 max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 50, 50, 16)   18448       add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 50, 50, 16)   64          conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 50, 50, 128)  18560       batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 50, 50, 128)  147584      add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 50, 50, 128)  512         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 50, 50, 128)  512         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 25, 25, 128)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 25, 25, 128)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 25, 25, 128)  0           max_pooling2d_3[0][0]            \n",
      "                                                                 max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 25, 25, 16)   18448       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 25, 25, 16)   64          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 25, 25, 128)  18560       batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 25, 25, 128)  147584      add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 25, 25, 128)  512         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 25, 25, 128)  512         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 12, 12, 128)  0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 12, 12, 128)  0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 12, 12, 128)  0           max_pooling2d_5[0][0]            \n",
      "                                                                 max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 12, 12, 16)   18448       add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 12, 12, 16)   64          conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 12, 12, 128)  18560       batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 12, 12, 128)  147584      add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 12, 12, 128)  512         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 12, 12, 128)  512         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 6, 6, 128)    0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)  (None, 6, 6, 128)    0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 6, 6, 128)    0           max_pooling2d_7[0][0]            \n",
      "                                                                 max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 4608)         0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 4608)         0           flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 32)           147488      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            33          dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 766,369\n",
      "Trainable params: 764,129\n",
      "Non-trainable params: 2,240\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = tuner.hypermodel.build(best_hps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "200/200 [==============================] - 60s 298ms/step - loss: 0.7139 - acc: 0.5653 - val_loss: 0.6935 - val_acc: 0.5000\n",
      "Epoch 2/100\n",
      "200/200 [==============================] - 59s 297ms/step - loss: 0.6526 - acc: 0.6136 - val_loss: 0.6637 - val_acc: 0.5818\n",
      "Epoch 3/100\n",
      "200/200 [==============================] - 59s 297ms/step - loss: 0.6477 - acc: 0.6067 - val_loss: 0.6344 - val_acc: 0.6218\n",
      "Epoch 4/100\n",
      "200/200 [==============================] - 59s 297ms/step - loss: 0.6296 - acc: 0.6533 - val_loss: 0.7369 - val_acc: 0.5880\n",
      "Epoch 5/100\n",
      "200/200 [==============================] - 60s 298ms/step - loss: 0.6204 - acc: 0.6591 - val_loss: 0.7183 - val_acc: 0.5842\n",
      "Epoch 6/100\n",
      "200/200 [==============================] - 60s 298ms/step - loss: 0.6190 - acc: 0.6689 - val_loss: 0.5955 - val_acc: 0.6810\n",
      "Epoch 7/100\n",
      "200/200 [==============================] - 60s 298ms/step - loss: 0.5968 - acc: 0.6820 - val_loss: 0.8608 - val_acc: 0.6314\n",
      "Epoch 8/100\n",
      "200/200 [==============================] - 60s 298ms/step - loss: 0.5829 - acc: 0.7081 - val_loss: 0.5958 - val_acc: 0.6822\n",
      "Epoch 9/100\n",
      "200/200 [==============================] - 60s 298ms/step - loss: 0.5683 - acc: 0.7177 - val_loss: 0.7292 - val_acc: 0.6554\n",
      "Epoch 10/100\n",
      "200/200 [==============================] - 59s 297ms/step - loss: 0.5474 - acc: 0.7369 - val_loss: 0.5372 - val_acc: 0.7464\n",
      "Epoch 11/100\n",
      "200/200 [==============================] - 60s 299ms/step - loss: 0.5460 - acc: 0.7328 - val_loss: 0.5909 - val_acc: 0.6848\n",
      "Epoch 12/100\n",
      "200/200 [==============================] - 60s 299ms/step - loss: 0.5163 - acc: 0.7609 - val_loss: 0.5112 - val_acc: 0.7576\n",
      "Epoch 13/100\n",
      "200/200 [==============================] - 60s 299ms/step - loss: 0.4899 - acc: 0.7811 - val_loss: 0.4930 - val_acc: 0.7678\n",
      "Epoch 14/100\n",
      "200/200 [==============================] - 60s 299ms/step - loss: 0.4830 - acc: 0.7761 - val_loss: 0.6727 - val_acc: 0.7054\n",
      "Epoch 15/100\n",
      "200/200 [==============================] - 60s 299ms/step - loss: 0.4740 - acc: 0.7877 - val_loss: 0.4913 - val_acc: 0.7694\n",
      "Epoch 16/100\n",
      "200/200 [==============================] - 60s 301ms/step - loss: 0.4570 - acc: 0.7958 - val_loss: 0.6175 - val_acc: 0.7178\n",
      "Epoch 17/100\n",
      "200/200 [==============================] - 60s 298ms/step - loss: 0.4335 - acc: 0.8111 - val_loss: 0.4824 - val_acc: 0.7752\n",
      "Epoch 18/100\n",
      "200/200 [==============================] - 60s 299ms/step - loss: 0.4006 - acc: 0.8278 - val_loss: 0.4195 - val_acc: 0.8076\n",
      "Epoch 19/100\n",
      "200/200 [==============================] - 60s 298ms/step - loss: 0.3832 - acc: 0.8308 - val_loss: 0.4369 - val_acc: 0.7968\n",
      "Epoch 20/100\n",
      "200/200 [==============================] - 60s 299ms/step - loss: 0.3492 - acc: 0.8455 - val_loss: 0.4216 - val_acc: 0.7940\n",
      "Epoch 21/100\n",
      "200/200 [==============================] - 60s 299ms/step - loss: 0.3272 - acc: 0.8587 - val_loss: 0.4058 - val_acc: 0.8200\n",
      "Epoch 22/100\n",
      "200/200 [==============================] - 60s 299ms/step - loss: 0.3085 - acc: 0.8677 - val_loss: 0.2600 - val_acc: 0.8830\n",
      "Epoch 23/100\n",
      "200/200 [==============================] - 60s 299ms/step - loss: 0.2963 - acc: 0.8775 - val_loss: 0.3503 - val_acc: 0.8240\n",
      "Epoch 24/100\n",
      "200/200 [==============================] - 60s 298ms/step - loss: 0.2804 - acc: 0.8784 - val_loss: 0.2574 - val_acc: 0.8900\n",
      "Epoch 25/100\n",
      "200/200 [==============================] - 60s 299ms/step - loss: 0.2721 - acc: 0.8845 - val_loss: 0.2878 - val_acc: 0.8708\n",
      "Epoch 26/100\n",
      "200/200 [==============================] - 60s 299ms/step - loss: 0.2484 - acc: 0.8959 - val_loss: 0.2629 - val_acc: 0.8812\n",
      "Epoch 27/100\n",
      "200/200 [==============================] - 60s 299ms/step - loss: 0.2556 - acc: 0.8906 - val_loss: 0.3561 - val_acc: 0.8332\n",
      "Epoch 28/100\n",
      "200/200 [==============================] - 60s 299ms/step - loss: 0.2338 - acc: 0.8984 - val_loss: 0.2264 - val_acc: 0.9054\n",
      "Epoch 29/100\n",
      "200/200 [==============================] - 60s 299ms/step - loss: 0.2252 - acc: 0.9077 - val_loss: 0.2107 - val_acc: 0.9102\n",
      "Epoch 30/100\n",
      "200/200 [==============================] - 60s 298ms/step - loss: 0.2279 - acc: 0.9072 - val_loss: 0.2333 - val_acc: 0.8992\n",
      "Epoch 31/100\n",
      "200/200 [==============================] - 60s 302ms/step - loss: 0.2193 - acc: 0.9078 - val_loss: 0.3236 - val_acc: 0.8548\n",
      "Epoch 32/100\n",
      "200/200 [==============================] - 60s 300ms/step - loss: 0.2242 - acc: 0.9056 - val_loss: 0.2361 - val_acc: 0.9074\n",
      "Epoch 33/100\n",
      "200/200 [==============================] - 60s 299ms/step - loss: 0.2195 - acc: 0.9106 - val_loss: 0.2754 - val_acc: 0.8734\n",
      "Epoch 34/100\n",
      "200/200 [==============================] - 60s 300ms/step - loss: 0.1949 - acc: 0.9209 - val_loss: 0.2325 - val_acc: 0.9042\n",
      "Epoch 35/100\n",
      "200/200 [==============================] - 60s 300ms/step - loss: 0.1956 - acc: 0.9170 - val_loss: 0.1708 - val_acc: 0.9274\n",
      "Epoch 36/100\n",
      "200/200 [==============================] - 60s 300ms/step - loss: 0.2009 - acc: 0.9172 - val_loss: 0.2431 - val_acc: 0.8980\n",
      "Epoch 37/100\n",
      "200/200 [==============================] - 60s 300ms/step - loss: 0.1955 - acc: 0.9195 - val_loss: 0.1919 - val_acc: 0.9184\n",
      "Epoch 38/100\n",
      "200/200 [==============================] - 61s 303ms/step - loss: 0.1876 - acc: 0.9223 - val_loss: 0.1783 - val_acc: 0.9234\n",
      "Epoch 39/100\n",
      "200/200 [==============================] - 61s 305ms/step - loss: 0.1811 - acc: 0.9256 - val_loss: 0.1512 - val_acc: 0.9384\n",
      "Epoch 40/100\n",
      "200/200 [==============================] - 60s 299ms/step - loss: 0.1890 - acc: 0.9223 - val_loss: 0.1872 - val_acc: 0.9308\n",
      "Epoch 41/100\n",
      "200/200 [==============================] - 60s 298ms/step - loss: 0.1667 - acc: 0.9320 - val_loss: 0.1508 - val_acc: 0.9388\n",
      "Epoch 42/100\n",
      "200/200 [==============================] - 60s 300ms/step - loss: 0.1797 - acc: 0.9241 - val_loss: 0.1937 - val_acc: 0.9208\n",
      "Epoch 43/100\n",
      "200/200 [==============================] - 60s 300ms/step - loss: 0.1804 - acc: 0.9295 - val_loss: 0.2680 - val_acc: 0.8804\n",
      "Epoch 44/100\n",
      "200/200 [==============================] - 60s 299ms/step - loss: 0.1693 - acc: 0.9287 - val_loss: 0.1853 - val_acc: 0.9210\n",
      "Epoch 45/100\n",
      "200/200 [==============================] - 60s 300ms/step - loss: 0.1686 - acc: 0.9297 - val_loss: 0.1387 - val_acc: 0.9428\n",
      "Epoch 46/100\n",
      "200/200 [==============================] - 60s 302ms/step - loss: 0.1603 - acc: 0.9327 - val_loss: 0.1344 - val_acc: 0.9448\n",
      "Epoch 47/100\n",
      "200/200 [==============================] - 60s 300ms/step - loss: 0.1595 - acc: 0.9314 - val_loss: 0.1391 - val_acc: 0.9458\n",
      "Epoch 48/100\n",
      "200/200 [==============================] - 60s 300ms/step - loss: 0.1640 - acc: 0.9302 - val_loss: 0.1539 - val_acc: 0.9374\n",
      "Epoch 49/100\n",
      "200/200 [==============================] - 60s 300ms/step - loss: 0.1633 - acc: 0.9366 - val_loss: 0.1216 - val_acc: 0.9488\n",
      "Epoch 50/100\n",
      "200/200 [==============================] - 60s 299ms/step - loss: 0.1702 - acc: 0.9280 - val_loss: 0.2226 - val_acc: 0.9120\n",
      "Epoch 51/100\n",
      "200/200 [==============================] - 60s 300ms/step - loss: 0.1517 - acc: 0.9377 - val_loss: 0.1384 - val_acc: 0.9476\n",
      "Epoch 52/100\n",
      "200/200 [==============================] - 60s 299ms/step - loss: 0.1620 - acc: 0.9353 - val_loss: 0.1530 - val_acc: 0.9354\n",
      "Epoch 53/100\n",
      "200/200 [==============================] - 60s 299ms/step - loss: 0.1541 - acc: 0.9380 - val_loss: 0.1287 - val_acc: 0.9438\n",
      "Epoch 54/100\n",
      "200/200 [==============================] - 60s 300ms/step - loss: 0.1488 - acc: 0.9381 - val_loss: 0.1474 - val_acc: 0.9436\n",
      "Epoch 55/100\n",
      "200/200 [==============================] - 60s 301ms/step - loss: 0.1561 - acc: 0.9348 - val_loss: 0.1491 - val_acc: 0.9366\n",
      "Epoch 56/100\n",
      "200/200 [==============================] - 60s 300ms/step - loss: 0.1522 - acc: 0.9411 - val_loss: 0.1985 - val_acc: 0.9138\n",
      "Epoch 57/100\n",
      "200/200 [==============================] - 60s 300ms/step - loss: 0.1391 - acc: 0.9444 - val_loss: 0.1674 - val_acc: 0.9300\n",
      "Epoch 58/100\n",
      "200/200 [==============================] - 60s 300ms/step - loss: 0.1421 - acc: 0.9450 - val_loss: 0.1266 - val_acc: 0.9516\n",
      "Epoch 59/100\n",
      "200/200 [==============================] - 60s 302ms/step - loss: 0.1420 - acc: 0.9430 - val_loss: 0.1262 - val_acc: 0.9476\n",
      "Epoch 60/100\n",
      "200/200 [==============================] - 60s 300ms/step - loss: 0.1423 - acc: 0.9427 - val_loss: 0.1454 - val_acc: 0.9376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/100\n",
      "200/200 [==============================] - 60s 299ms/step - loss: 0.1430 - acc: 0.9422 - val_loss: 0.1992 - val_acc: 0.9196\n",
      "Epoch 62/100\n",
      "200/200 [==============================] - 60s 301ms/step - loss: 0.1418 - acc: 0.9414 - val_loss: 0.1280 - val_acc: 0.9478\n",
      "Epoch 63/100\n",
      "200/200 [==============================] - 60s 300ms/step - loss: 0.1307 - acc: 0.9459 - val_loss: 0.1510 - val_acc: 0.9394\n",
      "Epoch 64/100\n",
      "200/200 [==============================] - 60s 300ms/step - loss: 0.1479 - acc: 0.9381 - val_loss: 0.1300 - val_acc: 0.9482\n",
      "Epoch 65/100\n",
      "200/200 [==============================] - 60s 301ms/step - loss: 0.1358 - acc: 0.9470 - val_loss: 0.1754 - val_acc: 0.9262\n",
      "Epoch 66/100\n",
      "200/200 [==============================] - 60s 300ms/step - loss: 0.1342 - acc: 0.9434 - val_loss: 0.2202 - val_acc: 0.9014\n",
      "Epoch 67/100\n",
      "200/200 [==============================] - 60s 299ms/step - loss: 0.1303 - acc: 0.9495 - val_loss: 0.1269 - val_acc: 0.9482\n",
      "Epoch 68/100\n",
      "200/200 [==============================] - 60s 300ms/step - loss: 0.1398 - acc: 0.9427 - val_loss: 0.1557 - val_acc: 0.9362\n",
      "Epoch 69/100\n",
      "200/200 [==============================] - 60s 300ms/step - loss: 0.1269 - acc: 0.9466 - val_loss: 0.2178 - val_acc: 0.9160\n",
      "Epoch 70/100\n",
      "200/200 [==============================] - 60s 300ms/step - loss: 0.1366 - acc: 0.9450 - val_loss: 0.1316 - val_acc: 0.9454\n",
      "Epoch 71/100\n",
      "200/200 [==============================] - 60s 300ms/step - loss: 0.1250 - acc: 0.9519 - val_loss: 0.1325 - val_acc: 0.9458\n",
      "Epoch 72/100\n",
      "200/200 [==============================] - 60s 302ms/step - loss: 0.1268 - acc: 0.9502 - val_loss: 0.1273 - val_acc: 0.9492\n",
      "Epoch 73/100\n",
      "200/200 [==============================] - 60s 302ms/step - loss: 0.1211 - acc: 0.9514 - val_loss: 0.1364 - val_acc: 0.9474\n",
      "Epoch 74/100\n",
      "200/200 [==============================] - 60s 301ms/step - loss: 0.1209 - acc: 0.9536 - val_loss: 0.1322 - val_acc: 0.9474\n",
      "Epoch 75/100\n",
      "200/200 [==============================] - 60s 300ms/step - loss: 0.1180 - acc: 0.9530 - val_loss: 0.1382 - val_acc: 0.9442\n",
      "Epoch 76/100\n",
      "200/200 [==============================] - 60s 300ms/step - loss: 0.1305 - acc: 0.9481 - val_loss: 0.2079 - val_acc: 0.9108\n",
      "Epoch 77/100\n",
      "200/200 [==============================] - 60s 301ms/step - loss: 0.1253 - acc: 0.9508 - val_loss: 0.1040 - val_acc: 0.9586\n",
      "Epoch 78/100\n",
      "200/200 [==============================] - 60s 300ms/step - loss: 0.1178 - acc: 0.9525 - val_loss: 0.1562 - val_acc: 0.9356\n",
      "Epoch 79/100\n",
      "200/200 [==============================] - 60s 302ms/step - loss: 0.1148 - acc: 0.9548 - val_loss: 0.1517 - val_acc: 0.9396\n",
      "Epoch 80/100\n",
      "200/200 [==============================] - 60s 301ms/step - loss: 0.1194 - acc: 0.9541 - val_loss: 0.1020 - val_acc: 0.9582\n",
      "Epoch 81/100\n",
      "200/200 [==============================] - 60s 301ms/step - loss: 0.1252 - acc: 0.9481 - val_loss: 0.1302 - val_acc: 0.9486\n",
      "Epoch 82/100\n",
      "200/200 [==============================] - 60s 301ms/step - loss: 0.1113 - acc: 0.9539 - val_loss: 0.1132 - val_acc: 0.9526\n",
      "Epoch 83/100\n",
      "200/200 [==============================] - 60s 301ms/step - loss: 0.1098 - acc: 0.9566 - val_loss: 0.0966 - val_acc: 0.9606\n",
      "Epoch 84/100\n",
      "200/200 [==============================] - 60s 300ms/step - loss: 0.1114 - acc: 0.9545 - val_loss: 0.1253 - val_acc: 0.9510\n",
      "Epoch 85/100\n",
      "200/200 [==============================] - 60s 300ms/step - loss: 0.1112 - acc: 0.9553 - val_loss: 0.1661 - val_acc: 0.9320\n",
      "Epoch 86/100\n",
      "200/200 [==============================] - 60s 300ms/step - loss: 0.1183 - acc: 0.9531 - val_loss: 0.1299 - val_acc: 0.9466\n",
      "Epoch 87/100\n",
      "200/200 [==============================] - 60s 301ms/step - loss: 0.1066 - acc: 0.9563 - val_loss: 0.0963 - val_acc: 0.9620\n",
      "Epoch 88/100\n",
      "200/200 [==============================] - 60s 300ms/step - loss: 0.1003 - acc: 0.9591 - val_loss: 0.1097 - val_acc: 0.9560\n",
      "Epoch 89/100\n",
      "200/200 [==============================] - 60s 300ms/step - loss: 0.1024 - acc: 0.9611 - val_loss: 0.1330 - val_acc: 0.9448\n",
      "Epoch 90/100\n",
      "200/200 [==============================] - 60s 300ms/step - loss: 0.1166 - acc: 0.9550 - val_loss: 0.0967 - val_acc: 0.9592\n",
      "Epoch 91/100\n",
      "200/200 [==============================] - 60s 299ms/step - loss: 0.1108 - acc: 0.9567 - val_loss: 0.1040 - val_acc: 0.9610\n",
      "Epoch 92/100\n",
      "200/200 [==============================] - 60s 302ms/step - loss: 0.1158 - acc: 0.9541 - val_loss: 0.1051 - val_acc: 0.9544\n",
      "Epoch 93/100\n",
      "200/200 [==============================] - 60s 300ms/step - loss: 0.1127 - acc: 0.9550 - val_loss: 0.1833 - val_acc: 0.9242\n",
      "Epoch 94/100\n",
      "200/200 [==============================] - 60s 300ms/step - loss: 0.1065 - acc: 0.9563 - val_loss: 0.1036 - val_acc: 0.9612\n",
      "Epoch 95/100\n",
      "200/200 [==============================] - 60s 301ms/step - loss: 0.1017 - acc: 0.9600 - val_loss: 0.1279 - val_acc: 0.9458\n",
      "Epoch 96/100\n",
      "200/200 [==============================] - 60s 300ms/step - loss: 0.0995 - acc: 0.9598 - val_loss: 0.1131 - val_acc: 0.9572\n",
      "Epoch 97/100\n",
      "200/200 [==============================] - 60s 300ms/step - loss: 0.1038 - acc: 0.9594 - val_loss: 0.1652 - val_acc: 0.9358\n",
      "Epoch 98/100\n",
      "200/200 [==============================] - 60s 300ms/step - loss: 0.1166 - acc: 0.9525 - val_loss: 0.1420 - val_acc: 0.9442\n",
      "Epoch 99/100\n",
      "200/200 [==============================] - 60s 300ms/step - loss: 0.1097 - acc: 0.9572 - val_loss: 0.1006 - val_acc: 0.9578\n",
      "Epoch 100/100\n",
      "200/200 [==============================] - 60s 300ms/step - loss: 0.1055 - acc: 0.9553 - val_loss: 0.1209 - val_acc: 0.9546\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c9bdda9e50>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data ,validation_data=test_data ,epochs=100 ,steps_per_epoch=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"Xcep_tuner.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
